# -*- coding: utf-8 -*-
"""Cinematic Genius : Movie Recommender System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z6Y4uNvyg0Uvu1t2EgtUvXrv3ttMq5pm
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt

# Load datasets
movies = pd.read_csv('movies_metadata.csv', low_memory=False)
ratings = pd.read_csv('ratings_small.csv')
keywords = pd.read_csv('keywords.csv')

# Display basic info
print(movies.info())
print(ratings.info())
print(keywords.info())

# Handle missing values if any
movies.dropna(subset=['title'], inplace=True)  # Drop rows where title is missing
ratings.dropna(subset=['rating'], inplace=True)  # Drop rows where rating is missing
keywords.dropna(subset=['keywords'], inplace=True)  # Drop rows where keywords are missing

# Convert column names to lowercase for consistency
movies.columns = [col.lower() for col in movies.columns]
ratings.columns = [col.lower() for col in ratings.columns]
keywords.columns = [col.lower() for col in keywords.columns]

# Display the first few rows of the datasets
print(movies.head())
print(ratings.head())
print(keywords.head())

# Basic statistics
print(ratings.describe())

# Clean movie_id column
movies['id'] = pd.to_numeric(movies['id'], errors='coerce')
ratings = ratings[ratings['movieid'].isin(movies['id'])]

# Merge datasets for further analysis
movies_keywords = pd.merge(movies, keywords, left_on='id', right_on='id')

"""Distribution of Movie Ratings

"""

plt.figure(figsize=(12, 6))
sns.histplot(ratings['rating'], bins=30, kde=True, color='blue')
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Number of Ratings per Movie"""

plt.figure(figsize=(12, 6))
ratings_per_movie = ratings.groupby('movieid').count()['rating']
sns.histplot(ratings_per_movie, bins=50, kde=True, color='green')
plt.title('Number of Ratings per Movie')
plt.xlabel('Number of Ratings')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Top 10 Movies with Most Ratings"""

top_rated_movies = ratings_per_movie.sort_values(ascending=False).head(10)
top_movies = movies[movies['id'].isin(top_rated_movies.index)]
plt.figure(figsize=(12, 8))
sns.barplot(x='id', y='title', data=top_movies, palette='viridis')
plt.title('Top 10 Movies with Most Ratings')
plt.xlabel('Number of Ratings')
plt.ylabel('Movie Title')
plt.grid(True)
plt.show()

"""Average Ratings per Movie"""

average_ratings = ratings.groupby('movieid').mean()['rating']
plt.figure(figsize=(12, 6))
sns.histplot(average_ratings, bins=30, kde=True, color='purple')
plt.title('Distribution of Average Ratings per Movie')
plt.xlabel('Average Rating')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Genre Popularity by Movie Count"""

import ast # Import the ast module

# Extract genres and count
movies['genres'] = movies['genres'].fillna('[]').apply(ast.literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
genre_count = pd.DataFrame(movies['genres'].explode().value_counts().reset_index())
genre_count.columns = ['Genre', 'Count']

plt.figure(figsize=(12, 8))
sns.barplot(x='Count', y='Genre', data=genre_count, palette='rocket')
plt.title('Genre Popularity by Movie Count')
plt.xlabel('Count')
plt.ylabel('Genre')
plt.grid(True)
plt.show()

"""Distribution of Movie Lengths"""

movies['runtime'] = pd.to_numeric(movies['runtime'], errors='coerce')
plt.figure(figsize=(12, 6))
sns.histplot(movies['runtime'].dropna(), bins=30, kde=True, color='orange')
plt.title('Distribution of Movie Lengths')
plt.xlabel('Runtime (minutes)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""Word Cloud of Keywords"""

from wordcloud import WordCloud
import ast

# Combine all keywords into one large string
all_keywords = ' '.join(keywords['keywords'].apply(lambda x: ' '.join([d['name'] for d in ast.literal_eval(x)]))) # Extract the 'name' values from the dictionaries

# Generate a word cloud
plt.figure(figsize=(12, 8))
wordcloud = WordCloud(width=1000, height=800, background_color='white').generate(all_keywords)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Keywords')
plt.show()

"""Correlation Heatmap between Features"""

# Selecting numeric columns for correlation analysis
numeric_features = movies.select_dtypes(include=[np.number])
corr_matrix = numeric_features.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap between Features')
plt.show()

"""Movie Ratings by Year"""

# Extract release year and plot
movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year
ratings_by_year = ratings.merge(movies[['id', 'release_year']], left_on='movieid', right_on='id')
average_rating_by_year = ratings_by_year.groupby('release_year')['rating'].mean()

plt.figure(figsize=(12, 6))
sns.lineplot(x=average_rating_by_year.index, y=average_rating_by_year.values)
plt.title('Average Movie Ratings by Year')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.grid(True)
plt.show()

"""Number of Movies Released Per Year"""

movies_by_year = movies['release_year'].value_counts().sort_index()

plt.figure(figsize=(12, 6))
sns.lineplot(x=movies_by_year.index, y=movies_by_year.values, color='red')
plt.title('Number of Movies Released Per Year')
plt.xlabel('Year')
plt.ylabel('Number of Movies')
plt.grid(True)
plt.show()

""" Building and Enhancing the Recommender System"""

# Load the data into Surprise format
!pip install scikit-surprise
from surprise import Reader, Dataset

# Load the data into Surprise format
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings[['userid', 'movieid', 'rating']], reader)

# Load the data into Surprise format
!pip install scikit-surprise
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate



svd = SVD()
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Train the model
trainset = data.build_full_trainset()
svd.fit(trainset)

"""Hybrid Recommender System: Combining Collaborative and Content-Based Filtering

Import Necessary Libraries
"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split

# Load dataset
ratings = pd.read_csv('ratings_small.csv')
movies = pd.read_csv('movies_metadata.csv', low_memory=False)

movies['id'] = pd.to_numeric(movies['id'], errors='coerce')

ratings = ratings[ratings['movieId'].isin(movies['id'])]

# Prepare data for Surprise library
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Split the data into training and testing sets
trainset, testset = train_test_split(data, test_size=0.25)

# Train SVD model
svd = SVD()
svd.fit(trainset)

"""Predict the Rating"""

# Predict the rating for a specific user and movie
user_id = 1
movie_id = 10
rating_prediction = svd.predict(user_id, movie_id)
print(f"Predicted rating for user {user_id} and movie {movie_id}: {rating_prediction.est}")

"""Function to Recommend Top N Movies for a Given User"""

# Function to recommend top N movies for a given user
def recommend_movies(user_id, num_recommendations=10):
    movie_ids = movies['id'].dropna().unique()
    movie_ratings = [svd.predict(user_id, movie_id).est for movie_id in movie_ids]
    recommendations = pd.DataFrame({
        'movieid': movie_ids,
        'predicted_rating': movie_ratings
    })
    recommendations = recommendations.sort_values(by='predicted_rating', ascending=False)
    top_recommendations = recommendations.head(num_recommendations)
    top_recommendations = pd.merge(top_recommendations, movies[['id', 'title']], left_on='movieid', right_on='id')
    return top_recommendations[['title', 'predicted_rating']]

# Recommend top 10 movies for user with ID 1
recommendations = recommend_movies(1, 10)
print(recommendations)

"""Plotting Predicted Ratings for a Given User

"""

import matplotlib.pyplot as plt

# Function to plot the predicted ratings for a user
def plot_predicted_ratings(user_id):
    movie_ids = movies['id'].dropna().unique()
    movie_ratings = [svd.predict(user_id, movie_id).est for movie_id in movie_ids]

    # Create a DataFrame for plotting
    ratings_df = pd.DataFrame({
        'movie_id': movie_ids,
        'predicted_rating': movie_ratings
    }).sort_values(by='predicted_rating', ascending=False)

"""Create the Streamlit App"""

pip install streamlit

"""Create the Streamlit App:"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split

movies = pd.read_csv('movies_metadata.csv', low_memory=False)
ratings = pd.read_csv('ratings_small.csv')
keywords = pd.read_csv('keywords.csv')

movies['id'] = pd.to_numeric(movies['id'], errors='coerce')
ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')

# Drop any rows with missing 'id' or 'movieId'
movies = movies.dropna(subset=['id'])
ratings = ratings.dropna(subset=['movieId'])

movies['id'] = movies['id'].astype(int)
ratings['movieId'] = ratings['movieId'].astype(int)

movies = pd.merge(movies, keywords, left_on='id', right_on='id', how='left')

# Prepare data for Surprise model
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
trainset = data.build_full_trainset()

# Train SVD model
svd = SVD()
svd.fit(trainset)

# Streamlit app
st.title("Cinematic Genius: Movie Recommender system")

# Sidebar input for user ID
user_id = st.sidebar.number_input("Enter User ID", min_value=1, value=1, step=1)
num_recommendations = st.sidebar.number_input("Number of Recommendations", min_value=1, value=10, step=1)

# Function to recommend top N movies for a given user
def recommend_movies(user_id, num_recommendations=10):
    movie_ids = movies['id'].unique()
    movie_ratings = [svd.predict(user_id, movie_id).est for movie_id in movie_ids]
    recommendations = pd.DataFrame({
        'movieid': movie_ids,
        'predicted_rating': movie_ratings
    })
    recommendations = recommendations.sort_values(by='predicted_rating', ascending=False)
    top_recommendations = recommendations.head(num_recommendations)
    top_recommendations = pd.merge(top_recommendations, movies[['id', 'title']], left_on='movieid', right_on='id')
    return top_recommendations

# Function to plot the predicted ratings for a user
def plot_predicted_ratings(user_id, top_recommendations):
    plt.figure(figsize=(10, 6))
    plt.barh(top_recommendations['title'], top_recommendations['predicted_rating'], color='skyblue')
    plt.xlabel('Predicted Rating')
    plt.ylabel('Movie Title')
    plt.title(f'Top {num_recommendations} Predicted Ratings for User {user_id}')
    plt.gca().invert_yaxis()
    st.pyplot(plt)

if st.sidebar.button("Get Recommendations"):
    recommendations = recommend_movies(user_id, num_recommendations)
    st.write(f"Top {num_recommendations} movie recommendations for User {user_id}:")
    st.dataframe(recommendations[['title', 'predicted_rating']])
    plot_predicted_ratings(user_id, recommendations)

import pickle

import pickle

# Assuming 'movies' is the DataFrame you want to pickle
pickle.dump(movies, open('movies.pkl','wb'))

import pandas as pd

df = pd.read_csv('movies_metadata.csv')
# Access the 'id' column
movie_ids = df['id']

import pickle

pickle.dump(movie_ids.to_dict(), open('movie_dict.pkl', 'wb'))

import pandas as pd
import pickle


with open('movie_dict.pkl', 'rb') as f:
    movie_dict = pickle.load(f)

# Convert the 'id' column of the DataFrame to a list
movies_list = [{'id': value} for value in movie_dict['id'].tolist()]
movies_df = pd.DataFrame(movies_list)

print(movies_df.columns)
print(movies_df.head())

movies_df.to_pickle('/content/movie_dict.pkl')

from google.colab import files
files.download('/content/movie_dict.pkl')

import pandas as pd


metadata_df = pd.read_csv('movies_metadata.csv')


movies_df = pd.read_pickle('/content/movie_dict.pkl')


movies_df = movies_df.merge(metadata_df[['id', 'original_title']], on='id', how='left')


print(movies_df['original_title'])


movies_df.to_pickle('/content/movie_dict_with_titles.pkl')

import pickle
import numpy as np

similarity = np.array([[1, 0.5], [0.5, 1]])

pickle.dump(similarity, open('similarity.pkl', 'wb'))

